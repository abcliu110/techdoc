# 解决阿里云个人版镜像推送慢的问题

## 问题原因

1. **阿里云个人版限速**：带宽限制约 500KB/s - 1MB/s
2. **镜像太大**：eclipse-temurin:21-jre (200MB+) + JAR (50-100MB) = 300-500MB
3. **推送时间计算**：500MB ÷ 500KB/s = 1000秒 ≈ 16分钟（理想情况）
4. **实际更慢**：网络波动、重试、认证等因素，可能需要 30-60 分钟

## 解决方案对比

| 方案 | 推送时间 | 成本 | 难度 | 推荐度 |
|------|---------|------|------|--------|
| 1. 添加超时重试 | 30分钟 | 免费 | 低 | ⭐⭐⭐ |
| 2. 优化镜像大小 | 10-15分钟 | 免费 | 中 | ⭐⭐⭐⭐ |
| 3. 本地镜像仓库 | 1-2分钟 | 低 | 中 | ⭐⭐⭐⭐⭐ |
| 4. 阿里云企业版 | 5-10分钟 | 高 | 低 | ⭐⭐⭐ |
| 5. Harbor 私有仓库 | 1-2分钟 | 中 | 高 | ⭐⭐⭐⭐⭐ |

---

## 方案 1: 添加超时和重试（已实施）

### 修改点
```groovy
# 添加 30 分钟超时
timeout 1800 /kaniko/executor ...

# 启用压缩
--compression=gzip
--compression-level=6

# 添加重试
--push-retry=3
--image-fs-extract-retry=3

# 分开推送两个标签
先推送 :26，再推送 :latest
```

### 优点
- 无需额外配置
- 失败会自动重试

### 缺点
- 仍然很慢（30分钟）
- 可能仍会超时

---

## 方案 2: 优化镜像大小（推荐）

### 2.1 使用 Alpine 基础镜像

**对比：**
```
eclipse-temurin:21-jre        → 约 220MB
eclipse-temurin:21-jre-alpine → 约 170MB
节省：50MB，推送时间减少 30%
```

**修改 Dockerfile：**
```dockerfile
# 原来
FROM eclipse-temurin:21-jre

# 改为
FROM eclipse-temurin:21-jre-alpine

# 如果需要时区支持
RUN apk add --no-cache tzdata && \
    cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \
    apk del tzdata
```

### 2.2 使用 Spring Boot 分层构建

**原理：** 将 JAR 分层，依赖层不变时不需要重新推送

**修改 pom.xml：**
```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
            <configuration>
                <layers>
                    <enabled>true</enabled>
                </layers>
            </configuration>
        </plugin>
    </plugins>
</build>
```

**修改 Dockerfile：**
```dockerfile
FROM eclipse-temurin:21-jre-alpine as builder
WORKDIR /app
COPY target/*.jar app.jar
RUN java -Djarmode=layertools -jar app.jar extract

FROM eclipse-temurin:21-jre-alpine
WORKDIR /app

# 分层复制（依赖层变化少，可以利用缓存）
COPY --from=builder /app/dependencies/ ./
COPY --from=builder /app/spring-boot-loader/ ./
COPY --from=builder /app/snapshot-dependencies/ ./
COPY --from=builder /app/application/ ./

ENTRYPOINT ["java", "org.springframework.boot.loader.JarLauncher"]
```

**效果：**
- 首次推送：仍需 30 分钟
- 后续推送：只推送变化的层，约 5-10 分钟

### 2.3 启用 Kaniko 缓存

**修改 Jenkinsfile：**
```groovy
# 原来
--cache=false

# 改为（需要配置缓存仓库）
--cache=true
--cache-repo=${DOCKER_REGISTRY}/${DOCKER_NAMESPACE}/cache
```

**注意：** 需要在阿里云创建 cache 仓库

---

## 方案 3: 部署本地镜像仓库（最佳方案）

### 3.1 在 K8s 集群内部署 Docker Registry

```yaml
# registry-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: docker-registry
  namespace: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: docker-registry
  template:
    metadata:
      labels:
        app: docker-registry
    spec:
      containers:
      - name: registry
        image: registry:2
        ports:
        - containerPort: 5000
        volumeMounts:
        - name: registry-data
          mountPath: /var/lib/registry
        env:
        - name: REGISTRY_STORAGE_DELETE_ENABLED
          value: "true"
      volumes:
      - name: registry-data
        persistentVolumeClaim:
          claimName: registry-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: docker-registry
  namespace: jenkins
spec:
  selector:
    app: docker-registry
  ports:
  - port: 5000
    targetPort: 5000
  type: ClusterIP

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: registry-pvc
  namespace: jenkins
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
```

**部署：**
```bash
kubectl apply -f registry-deployment.yaml
```

### 3.2 修改 Jenkinsfile 使用本地仓库

```groovy
environment {
    // 本地镜像仓库（K8s 集群内）
    LOCAL_REGISTRY = 'docker-registry.jenkins.svc.cluster.local:5000'

    // 阿里云镜像仓库（用于最终存储）
    REMOTE_REGISTRY = 'crpi-csgbt2t7j15cj178.cn-hangzhou.personal.cr.aliyuncs.com'
}

stage('构建并推送到本地仓库') {
    steps {
        container('kaniko') {
            script {
                // 先推送到本地仓库（很快，1-2分钟）
                sh """
                    /kaniko/executor \
                        --context=${buildContext} \
                        --dockerfile=${dockerfilePath} \
                        --destination=${LOCAL_REGISTRY}/${moduleName}:${BUILD_NUMBER} \
                        --insecure \
                        --skip-tls-verify
                """
            }
        }
    }
}

stage('同步到阿里云（后台）') {
    steps {
        container('docker') {
            script {
                // 后台同步到阿里云（不阻塞流水线）
                sh """
                    # 从本地仓库拉取
                    docker pull ${LOCAL_REGISTRY}/${moduleName}:${BUILD_NUMBER}

                    # 重新打标签
                    docker tag ${LOCAL_REGISTRY}/${moduleName}:${BUILD_NUMBER} \
                        ${REMOTE_REGISTRY}/lgy-images/${moduleName}:${BUILD_NUMBER}

                    # 推送到阿里云（后台执行）
                    nohup docker push ${REMOTE_REGISTRY}/lgy-images/${moduleName}:${BUILD_NUMBER} &

                    echo "✓ 镜像已推送到本地仓库，正在后台同步到阿里云"
                """
            }
        }
    }
}
```

**优点：**
- 推送到本地仓库：1-2 分钟
- K8s 部署直接使用本地镜像：秒级
- 后台同步到阿里云：不阻塞流水线

**缺点：**
- 需要额外存储空间（50GB+）
- 需要维护本地仓库

---

## 方案 4: 使用 Harbor（企业级方案）

### 4.1 部署 Harbor

```bash
# 使用 Helm 部署 Harbor
helm repo add harbor https://helm.goharbor.io
helm install harbor harbor/harbor \
    --namespace harbor \
    --create-namespace \
    --set expose.type=nodePort \
    --set persistence.enabled=true \
    --set persistence.persistentVolumeClaim.registry.size=100Gi
```

### 4.2 配置镜像同步

Harbor 支持自动同步到阿里云：
1. 在 Harbor 中配置阿里云为远程仓库
2. 设置同步规则
3. Jenkins 推送到 Harbor，Harbor 自动同步到阿里云

**优点：**
- 企业级功能：镜像扫描、签名、RBAC
- 自动同步到多个仓库
- Web UI 管理

**缺点：**
- 部署复杂
- 资源消耗大

---

## 方案 5: 升级阿里云企业版

**价格：** 约 ¥300-500/月

**优点：**
- 带宽更大（10MB/s+）
- 推送时间：5-10 分钟
- 无需额外维护

**缺点：**
- 需要付费

---

## 立即行动建议

### 短期（今天就做）
1. ✅ **已完成**：添加超时和重试机制
2. **优化 Dockerfile**：改用 Alpine 基础镜像
3. **启用压缩**：减少传输数据量

### 中期（本周完成）
1. **部署本地 Docker Registry**：解决推送慢的根本问题
2. **配置镜像分层**：利用 Docker 缓存

### 长期（可选）
1. **部署 Harbor**：企业级镜像管理
2. **升级阿里云企业版**：如果预算允许

---

## 当前卡住的处理

如果当前构建已经卡住 30 分钟：

```bash
# 1. 取消当前构建
在 Jenkins 中点击 "Abort"

# 2. 检查镜像是否已经推送成功
docker pull crpi-csgbt2t7j15cj178.cn-hangzhou.personal.cr.aliyuncs.com/lgy-images/nms4cloud-pos4cloud:26

# 3. 如果已经推送成功，直接使用
# 如果没有，使用优化后的配置重新构建
```

---

## 监控推送进度

在 Kaniko 容器中添加进度监控：

```bash
# 在推送时查看网络流量
watch -n 1 'cat /proc/net/dev'

# 或者使用 iftop（需要安装）
iftop -i eth0
```
