<div id="chap-pod"></div>

[⬆️ 返回目录](#catalog)

## pod

在 Kubernetes（K8s）的世界里，**Pod** 和 **Service** 是两个最基础、最重要的概念。我们可以用一个**“公司部门”**的例子来直观理解它们。

---

### 1. Pod：干活的工人（最小调度单位）

如果你以前用过 Docker，你可能习惯直接运行“容器”。但在 K8s 中，你不能直接运行容器，必须运行 **Pod**。

- **定义**：Pod 是 K8s 中最小的部署单元。一个 Pod 里面可以包含一个或多个容器（通常就是一个）。
- **形象理解**：Pod 就像是**“一个工位”**。
  - 工位上有电脑、桌椅。
  - 容器就像是**“坐在工位上的工人”**。
  - 虽然工人是真正干活的，但公司管理的是“工位”。
- **关键特性——“短命”**：
  - Pod 是非持久的、临时的。
  - 如果一个 Pod 崩溃了，或者服务器重启了，K8s 会销毁它并创建一个新的。
  - **重点**：每一个新创建的 Pod 都会被分配一个新的 IP 地址。

---

### 2. Service：稳定的办事窗口（服务发现与负载均衡）

既然 Pod 的 IP 是随时会变的，那问题来了：如果你有一个前端网页要调用后端的 API，而后端的 Pod 每次重启 IP 都会变，前端怎么找它？

**Service 就是为了解决这个问题而诞生的。**

- **定义**：Service 定义了一组 Pod 的逻辑集合，并为这组 Pod 提供一个**固定的、永久的 IP 地址和 DNS 名称**。
- **形象理解**：Service 就像是公司的**“前台办事窗口”**（比如“人力资源部”）。
  - 不管人力资源部里具体是谁在干活（Pod），只要你走到“人力资源部”这个固定的窗口，就能办成事。
  - 哪怕里面的员工换了、工位搬了，这个窗口的部门牌号（Service IP）永远不变。
- **功能**：
  1.  **服务发现**：前端只需要记住 Service 的名字，不需要管后端 Pod 的 IP。
  2.  **负载均衡**：如果你有 3 个 Pod 都在干同样的活，Service 会自动把请求平均分配给这 3 个 Pod。

---

### 3. 两者的关系：如何找到对方？

Service 是怎么知道哪些 Pod 归它管呢？靠的是 **标签 (Labels)**。

1.  你给 Pod 贴上标签：`app: user-api`。
2.  你在 Service 里面写一个选择器 (Selector)：`选择所有标签为 app: user-api 的 Pod`。
3.  Service 就像一个**自动追踪器**，无论 Pod 怎么销毁重建，只要标签对上了，Service 就能瞬间抓到它。

---

### 4. 回到你之前的配置

在你的 `config.yaml` 中：

- **`cluster-cidr: 10.42.0.0/16`**：
  这是分配给所有 **Pod** 的 IP 池。每个工人（Pod）入职时，都会从这个段里领到一个临时的 IP。
- **`service-cidr: 10.43.0.0/16`**：
  这是分配给所有 **Service** 的 IP 池。每个部门窗口（Service）成立时，都会从这里领到一个固定的、永远不变的 IP。

---

### 总结对比

| 特性         | Pod                      | Service                  |
| :----------- | :----------------------- | :----------------------- |
| **角色**     | 真正的执行者（运行容器） | 流量的调度者（稳定入口） |
| **生命周期** | 短暂的，随时可能被替换   | 长久的，除非手动删除     |
| **IP 地址**  | 动态变化                 | 固定不变                 |
| **数量**     | 通常有很多个             | 通常每个功能模块一个     |
| **比喻**     | 具体的员工               | 部门的牌子/办公电话      |

**一句话总结：Pod 是负责“干活”的，由于它不稳定，所以我们需要 Service 作为一个“稳固的盾牌”挡在前面，让别人能永远找到这些干活的。**

这是一个非常深入且核心的问题。在 Kubernetes 中，**Service 并不是一个真实的“进程”或“程序”去调用 Pod**，它更像是一套**分布式的路由规则**。

我们可以从“逻辑关联”和“流量转发”两个层面来拆解这个架构。

---

### 1. 逻辑层面：它是怎么“找到” Pod 的？（Endpoints）

Service 和 Pod 之间并不是直接连接的，中间有一个关键的隐藏对象叫 **Endpoints**。

1.  **标签匹配**：你给 Service 定义了一个 `Selector`（选择器），比如 `app: web`。
2.  **自动发现**：Kubernetes 的控制面（Control Plane）会不停地扫描集群。一旦发现有个 Pod 的标签是 `app: web` 且状态正常（Healthy），就会把这个 Pod 的 **私有 IP 地址** 记在一个名为 **Endpoints** 的列表里。
3.  **实时更新**：如果 Pod 死了，IP 会从列表里删掉；新 Pod 启动了，IP 会自动加进去。

---

### 2. 架构层面：流量是怎么“过去”的？（kube-proxy）

这是最神奇的地方：**Service 的 IP（Cluster IP）其实是一个“虚幻”的 IP，在网络中根本找不到对应的网卡。**

真正干活的组件叫 **`kube-proxy`**，它运行在集群的每一个节点（Node）上。

#### 架构流程如下：

1.  **制定规则**：`kube-proxy` 会观察 Service 和 Endpoints 的变化。
2.  **修改内核**：每当有一个 Service 被创建，`kube-proxy` 就会在每个节点的**操作系统内核**（使用 `iptables` 或 `IPVS` 模块）里写下一行“密令”。
    - _密令内容大概是：_ “凡是看到目标 IP 是 `10.43.0.10`（Service IP）的流量，请立刻把它拦截下来，并随机改写为 `10.42.0.5`（Pod A 的 IP）或 `10.42.0.6`（Pod B 的 IP）。”
3.  **流量拦截**：
    - 当你的程序尝试访问 Service IP 时，请求刚离开程序、进入操作系统的网络协议栈，就被内核里的那些规则拦截了。
    - **内核直接在内存里完成了目标的转换（NAT）**，然后把请求发往真正的 Pod IP。

---

### 3. 图解架构

想象一下这个场景：

```text
客户端程序 (发起请求 -> 10.43.0.10)
      |
      v
[ 节点内核 (iptables/IPVS 规则) ]  <--- 这里的规则由 kube-proxy 维护
      |
      |-- (根据负载均衡算法选择一个 Pod)
      |
      +------> 发往 Pod A (10.42.0.5)
      |
      +------> 发往 Pod B (10.42.0.6)
```

**这种架构的精妙之处在于：**

- **没有单点故障**：Service 流量分发是在每个节点本地的内核里完成的，不需要经过一个中心负载均衡器。
- **性能极高**：因为是在内核层直接转发，没有经过用户态程序的“中转”，延迟极低。
- **解耦**：客户端程序永远只看那个稳定的 Service IP，至于后面 Pod 是生是死、有多少个，它完全不关心。

---

### 4. 回答你的疑问：“Service 调用 Pod？”

准确地说：**Service 并不主动“调用” Pod。**

Service 更像是一个**“交通路牌”**或者**“重定向规则”**。

- **Pod** 是终点站。
- **Service** 是那个永远不变的虚拟路牌。
- **kube-proxy** 是那个在背后不停修改路牌指向的人。

当你（客户端）尝试走向路牌指向的地址时，实际上是操作系统的网络层把你“导向”了正确的 Pod。

### 总结

这种架构叫做 **“基于客户端侧拦截的分布式负载均衡”**。

- **Service** 是逻辑定义。
- **Endpoints** 是 Pod IP 实时名单。
- **kube-proxy + 内核规则** 是真正的流量搬运工。

<!-- 跳转链接 -->

[⬆️ 返回目录](#catalog) | [文章开头 ➡️](#chap-pod)
