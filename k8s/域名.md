<div id="chap-domain"></div>

[⬆️ 返回目录](#catalog)

## 域名

这是一个非常棒的底层技术问题！理解了 Pod 内部的 DNS 机制，你就掌握了 Kubernetes 服务发现（Service Discovery）的“灵魂”。

作为资深管理员，我为你揭开这个过程背后的**三层技术架构**：

---

### 第一层：Pod 的“身份证”——`resolv.conf` 文件

当你启动一个 Pod 时，Kubernetes 的 **Kubelet** 进程会自动在容器的根目录下创建一个配置文件：`/etc/resolv.conf`。

你可以通过命令查看它：`kubectl exec <pod-name> -- cat /etc/resolv.conf`。你通常会看到类似这样的内容：

```conf
nameserver 10.43.0.10             # 这是 CoreDNS 的虚拟 IP
search nms4cloud.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

**技术原理**：

- **nameserver**：告诉 Pod，所有的 DNS 查询请求都发给 `10.43.0.10` 这个内部地址。
- **search**：这是**“短域名补全”**的关键。当你代码里访问 `mysql` 时，系统会自动尝试拼接后缀，比如查找 `mysql.nms4cloud.svc.cluster.local`。

---

### 第二层：集群的“图书馆”——CoreDNS

在 K8s 集群里，有一个默认运行的服务叫 **CoreDNS**（通常在 `kube-system` 命名空间）。

**技术原理**：

1.  **监听 API Server**：CoreDNS 就像一个勤快的管理员，它一直盯着 K8s 的 API Server。
2.  **实时更新账本**：每当你创建一个 Service（比如名为 `mysql` 的 Service），API Server 就会通知 CoreDNS。CoreDNS 就会在自己的内存数据库里记下一笔：
    - _“名字叫 `mysql.nms4cloud.svc.cluster.local` 的家伙，它的虚拟 IP（ClusterIP）是 `10.43.83.36`。”_
3.  **回答询问**：当 Pod 发起请求问“mysql 在哪？”时，CoreDNS 查表并返回 `10.43.83.36`。

---

### 第三层：域名的“全称逻辑”（FQDN）

为什么你在 `nms4cloud` 命名空间里可以直接写 `mysql`，而不用写长长的地址？这就是 `search` 域的功劳。

**全称解析流程（以访问 `mysql` 为例）**：

1.  Pod 询问：`mysql` 是谁？
2.  系统查看 `resolv.conf`，发现有 search 域，于是开始尝试拼接：
    - 第一次尝试：`mysql.nms4cloud.svc.cluster.local` -> **命中！** CoreDNS 返回 ClusterIP。
3.  **跨命名空间访问**：如果你在 `default` 命名空间想连 `nms4cloud` 里的数据库，你就必须写 `mysql.nms4cloud`（名字.命名空间），系统拼接后变成 `mysql.nms4cloud.svc.cluster.local` 才能命中。

---

### 核心技术点：DNS 只负责“翻译”，不负责“修路”

这是一个常见的误区，作为资深程序员必须分清：

1.  **DNS (CoreDNS)**：只负责把 `mysql` 这个**名字**翻译成 **`10.43.83.36` (ClusterIP)**。一旦翻译完成，DNS 的任务就结束了。
2.  **流量转发 (Kube-proxy)**：当你的 Java 代码拿到 `10.43.83.36` 这个 IP 发起真正的 TCP 连接时，是宿主机上的 **iptables** 或 **IPVS** 规则在起作用，把流量真正导向 MySQL 的 Pod IP（比如 `10.42.0.182`）。

---

### 总结：为什么昨天开启 `hostNetwork` 会出问题？

回到你昨天遇到的惨案，技术细节如下：

- **普通模式**：Pod 有自己的网络空间 $\rightarrow$ 使用 K8s 生成的 `resolv.conf` $\rightarrow$ 询问 CoreDNS $\rightarrow$ 认得 `mysql`。
- **hostNetwork 模式**：Pod 直接用宿主机的网卡 $\rightarrow$ 读取宿主机的 `/etc/resolv.conf`（通常指向运营商 DNS） $\rightarrow$ 询问运营商 $\rightarrow$ **运营商不认识 `mysql`** $\rightarrow$ **解析失败，程序崩溃**。

### 💡 资深管理员的建议：

如果你想在 Pod 内部通过 DNS 调用服务，**永远不要随意动 `hostNetwork`**。

如果你非要用 `hostNetwork`：

- 要么把 `dnsPolicy` 改为 `ClusterFirstWithHostNet`（强制它去找 CoreDNS）。
- 要么在代码里放弃域名，改用固定 IP（如 `127.0.0.1` 或宿主机 IP）。

**理解了这一套“自动补全 + 内存账本 + 流量转发”的流程，你就真正理解了 K8s 网络的精髓！**

这是一个非常棒的实操问题！作为管理员，理解如何查看和解读这个文件是排查网络问题的“基本功”。

回答你的两个疑问：

### 一、 `kubectl exec` 只能在物理机上执行吗？

**不一定。** 只要是安装了 `kubectl` 客户端，并且拥有集群访问权限（`kubeconfig` 文件）的机器都可以执行。

1.  **在物理机（宿主机/控制节点）上**：你可以直接打开终端执行。
2.  **在笔记本上**：如果你在笔记本上配置了 `kubectl`（连接到你的 K8s 集群），你也可以在笔记本上执行。
3.  **在 Rancher UI 界面（最简单）**：
    - 你不需要敲完整的 `kubectl exec` 命令。
    - 直接在 Rancher 的 Pod 列表里，点击 Pod 右侧的三个点，选择 **“Execute Shell” (执行 Shell)**。
    - 进入黑窗口后，直接输入：`cat /etc/resolv.conf`。

---

### 二、 容器里 `/etc/resolv.conf` 的具体内容是什么？

根据你 Pod 的网络配置不同，这个文件的内容会发生**剧变**。这也是为什么你昨天连不上数据库的根本原因。

#### 1. 标准 Pod (普通模式，没开 hostNetwork)

这是 K8s 最标准的状态。文件内容通常如下：

```conf
nameserver 10.43.0.10             # 指向集群内部 CoreDNS 的虚拟 IP
search nms4cloud.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

- **为什么能认出 `mysql`？** 当你访问 `mysql` 时，系统发现这不是一个全域名，于是会自动根据 `search` 里的后缀去问 `10.43.0.10`：“请问 `mysql.nms4cloud.svc.cluster.local` 在哪？”。CoreDNS 认识它，于是解析成功。

#### 2. 开启了 `hostNetwork: true` 的 Pod

当你开了主机网络，Pod 就“叛变”了，它不再看集群内部。内容通常如下：

```conf
nameserver 114.114.114.114        # 直接变成了你物理机（宿主机）上的 DNS
nameserver 8.8.8.8
# 没有任何 search 域，或者只有宿主机的 search 域
```

- **为什么认不出 `mysql`？** 当 Nacos 访问 `mysql` 时，它直接去问 `114.114.114.114`：“请问 `mysql` 在哪？”。外网 DNS 根本不认识你的私有服务，于是报错 `UnknownHostException`。

---

### 三、 深度总结：为什么你昨天改了 YAML 会出问题？

作为资深管理员，我为你复盘昨天的技术链路：

1.  你给 Nacos 加上了 `hostNetwork: true`。
2.  Kubelet 在启动 Nacos 时，直接把**宿主机的 DNS 配置**（如运营商的 114.114...）拷贝进了 Nacos 容器的 `/etc/resolv.conf`。
3.  Nacos 程序启动，读取配置：`MYSQL_SERVICE_HOST: mysql`。
4.  Nacos 尝试请求 DNS 解析 `mysql`。
5.  解析请求发给了 **运营商 DNS**，运营商返回：“查无此人”。
6.  Nacos 报错：`No DataSource set`（因为第一步获取连接就由于找不到主机而失败了）。

### 🛠️ 管理员排查小技巧：

如果你怀疑 DNS 有问题，进入容器执行这两条命令：

1.  `cat /etc/resolv.conf`：看 `nameserver` 是不是集群内部 IP（通常以 `10.43` 或 `10.96` 开头）。
2.  `nslookup mysql`：看能不能解析出 IP。
    - 如果提示 `command not found`，说明镜像太精简没装工具。
    - 如果解析失败，看报错是 `NXDOMAIN`（没这个名）还是 `Timeout`（连不上 DNS 服务器）。

**一句话总结：**
`/etc/resolv.conf` 就是 Pod 的“通讯录”。**开启 `hostNetwork` 会导致通讯录被替换成宿主机的外网通讯录，从而丢掉集群内部的所有联系人。**这就是你昨天最大的坑。

作为一个资深程序员和网络管理员，你提到的 **SSL-SAN** 和 **RKE2 的组件拓扑** 触及了 Kubernetes 最底层、也最容易让高级工程师“翻车”的领域。

我们分两部分来深度解析：

---

### 一、 SSL-SAN 的技术原理与影响范围

#### 1. 技术原理

**SAN (Subject Alternative Name)** 是 X.509 数字证书标准的一个扩展。

- **传统模式 (CN)**：以前证书只看 **Common Name**（比如 `example.com`）。如果你换个 IP 或子域名访问，浏览器就会报“证书不匹配”。
- **SAN 模式**：允许你在一个证书里塞进一堆“别名”。这些别名可以是 **DNS 域名**，也可以是 **IP 地址**。

**身份验证逻辑**：
当你连接 `https://192.168.1.114:6443`（RKE2 API Server）时，客户端（kubectl 或其他 Pod）会检查服务器发来的证书：

- “证书里的 SAN 列表包含 `192.168.1.114` 吗？”
- “包含 `kubernetes.default.svc` 吗？”
- 如果不包含，连接就会因 **TLS Handshake Error** 直接中断。

#### 2. 它只影响外部通信吗？

**绝对不是。** 在 RKE2/K8s 中，SSL-SAN 对**内部通信**至关重要：

- **节点加入**：新 Node 加入集群时，如果它通过负载均衡 IP 连接 Server，而该 IP 不在 SAN 列表里，Node 无法加入。
- **Pod 调 API**：Pod 内部通过环境变量 `KUBERNETES_SERVICE_HOST` 调用 API Server。如果证书里没写内部域名（如 `kubernetes`），Pod 里的程序（比如某些监控组件）会报错。
- **RKE2 特色**：RKE2 是为安全设计的，它默认所有的组件通信全走双向 TLS (mTLS)。如果你在安装 RKE2 时没配好 `tls-san` 参数，以后你换个域名或加个负载均衡器，整个集群的证书都要重签。

---

### 二、 RKE2 中：部署、Pod、服务发现的关系与先后逻辑

在 RKE2 环境下，这三者的关系是一个**“声明 -> 实例化 -> 路由”**的闭环过程。

#### 1. 逻辑关系（三位一体）

- **Deployment (工作负载)**：它是**“意图”**。你告诉 RKE2：“我要运行 3 个 MySQL，用这个镜像”。它不负责跑程序，只负责数数。
- **Pod (容器组)**：它是**“实体”**。是真正消耗 CPU 和内存的进程。每个 Pod 会从 RKE2 的 CNI 网络（如 Canal/Cilium）获取一个虚拟 IP（10.42.x.x）。
- **Service (服务发现)**：它是**“稳定入口”**。Pod 的 IP 经常变，Service 提供一个固定的 ClusterIP（10.43.x.x）或域名。

#### 2. 先后顺序：谁先谁后？

在 K8s 的“声明式”架构下，**创建顺序不重要，但生效顺序有逻辑：**

- **第一步：Deployment 提交 (控制面动作)**
  你 `kubectl apply` 之后，API Server 把需求存入 etcd。此时没有 Pod，也没有流量。
- **第二步：Pod 调度与拉起 (数据面动作)**
  Scheduler 选机器，Kubelet 拉镜像、起容器。
  _注意_：在这个阶段，Pod 还没准备好，它处于 `ContainerCreating`。
- **第三步：Service 关联 (控制面动作)**
  Service 会根据 `selector` 标签去搜索集群里所有符合条件的 Pod。
- **第四步：Endpoint 就绪 (真正的“先后”分水岭)**
  这是你昨天遇到问题的关键：**Pod 必须通过 Readiness Probe（就绪检查）**。只有 Pod 绿了，Service 才会把这个 Pod 的 IP 塞进它的转发列表（Endpoints）。
  - **先有 Pod 运行成功 $\rightarrow$ 后有 Service Target（红框不再空白）。**

---

### 三、 总结：将 SSL-SAN 与你的环境结合

作为管理员，你在 RKE2 中需要注意：

1.  **安装 RKE2 时**：如果你的服务器有多个 IP（比如外网 IP 和内网 IP），或者你打算在它前面挡一个 Nginx 负载均衡，你必须在 `config.yaml` 里写上：

    ```yaml
    tls-san:
      - "my-rke2.example.com"
      - "192.168.1.114"
      - "负载均衡IP"
    ```

    如果不写，你的 `kubectl` 从笔记本远程连过来时会报证书错误。

2.  **服务发现的“先后”陷阱**：
    你昨天 Nacos 连不上 MySQL，是因为：
    - Deployment 起 Pod 了。
    - 但 Pod 的标签没对上，或者 Pod 没 Ready。
    - 导致 Service 后面是空的。
    - 结果：Nacos 在启动时去查 DNS 没查到，或者查到了一个无法访问的 IP。

**资深工程师的思维模型：**

- **SSL-SAN** 解决的是“**能不能进门**”（安全连接）。
- **Deployment/Pod/Service** 解决的是“**人在不在、怎么找到人**”（路由发现）。

如果你在 RKE2 里遇到 `x509: certificate is valid for xxx, not yyy`，那是 **SAN** 没配对。如果你遇到 `No DataSource set` 或 `UnknownHost`，那是 **Service/Pod 关联** 没对上。这两者共同构成了 K8s 网络的一硬一软两大支柱。

这是一个非常棒的实操问题！作为管理员，理解如何查看和解读这个文件是排查网络问题的“基本功”。

回答你的两个疑问：

### 一、 `kubectl exec` 只能在物理机上执行吗？

**不一定。** 只要是安装了 `kubectl` 客户端，并且拥有集群访问权限（`kubeconfig` 文件）的机器都可以执行。

1.  **在物理机（宿主机/控制节点）上**：你可以直接打开终端执行。
2.  **在笔记本上**：如果你在笔记本上配置了 `kubectl`（连接到你的 K8s 集群），你也可以在笔记本上执行。
3.  **在 Rancher UI 界面（最简单）**：
    - 你不需要敲完整的 `kubectl exec` 命令。
    - 直接在 Rancher 的 Pod 列表里，点击 Pod 右侧的三个点，选择 **“Execute Shell” (执行 Shell)**。
    - 进入黑窗口后，直接输入：`cat /etc/resolv.conf`。

---

### 二、 容器里 `/etc/resolv.conf` 的具体内容是什么？

根据你 Pod 的网络配置不同，这个文件的内容会发生**剧变**。这也是为什么你昨天连不上数据库的根本原因。

#### 1. 标准 Pod (普通模式，没开 hostNetwork)

这是 K8s 最标准的状态。文件内容通常如下：

```conf
nameserver 10.43.0.10             # 指向集群内部 CoreDNS 的虚拟 IP
search nms4cloud.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

- **为什么能认出 `mysql`？** 当你访问 `mysql` 时，系统发现这不是一个全域名，于是会自动根据 `search` 里的后缀去问 `10.43.0.10`：“请问 `mysql.nms4cloud.svc.cluster.local` 在哪？”。CoreDNS 认识它，于是解析成功。

#### 2. 开启了 `hostNetwork: true` 的 Pod

当你开了主机网络，Pod 就“叛变”了，它不再看集群内部。内容通常如下：

```conf
nameserver 114.114.114.114        # 直接变成了你物理机（宿主机）上的 DNS
nameserver 8.8.8.8
# 没有任何 search 域，或者只有宿主机的 search 域
```

- **为什么认不出 `mysql`？** 当 Nacos 访问 `mysql` 时，它直接去问 `114.114.114.114`：“请问 `mysql` 在哪？”。外网 DNS 根本不认识你的私有服务，于是报错 `UnknownHostException`。

---

### 三、 深度总结：为什么你昨天改了 YAML 会出问题？

作为资深管理员，我为你复盘昨天的技术链路：

1.  你给 Nacos 加上了 `hostNetwork: true`。
2.  Kubelet 在启动 Nacos 时，直接把**宿主机的 DNS 配置**（如运营商的 114.114...）拷贝进了 Nacos 容器的 `/etc/resolv.conf`。
3.  Nacos 程序启动，读取配置：`MYSQL_SERVICE_HOST: mysql`。
4.  Nacos 尝试请求 DNS 解析 `mysql`。
5.  解析请求发给了 **运营商 DNS**，运营商返回：“查无此人”。
6.  Nacos 报错：`No DataSource set`（因为第一步获取连接就由于找不到主机而失败了）。

### 🛠️ 管理员排查小技巧：

如果你怀疑 DNS 有问题，进入容器执行这两条命令：

1.  `cat /etc/resolv.conf`：看 `nameserver` 是不是集群内部 IP（通常以 `10.43` 或 `10.96` 开头）。
2.  `nslookup mysql`：看能不能解析出 IP。
    - 如果提示 `command not found`，说明镜像太精简没装工具。
    - 如果解析失败，看报错是 `NXDOMAIN`（没这个名）还是 `Timeout`（连不上 DNS 服务器）。

**一句话总结：**
`/etc/resolv.conf` 就是 Pod 的“通讯录”。**开启 `hostNetwork` 会导致通讯录被替换成宿主机的外网通讯录，从而丢掉集群内部的所有联系人。**这就是你昨天最大的坑。

<!-- 跳转链接 -->

[⬆️ 返回目录](#catalog) | [文章开头 ➡️](#chap-domain)
